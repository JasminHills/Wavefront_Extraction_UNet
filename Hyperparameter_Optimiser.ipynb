{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example of network outputs\n",
    "\n",
    "# Global import\n",
    "import sys\n",
    "sys.path.insert(0, '../pre_built')\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from dataset import *\n",
    "import sys\n",
    "from train import *\n",
    "from lr_analyzer import *\n",
    "from criterion import *\n",
    "from Unet import UNet\n",
    "from dataset import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example of network outputs\n",
    "\n",
    "# Global import\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from dataset import *\n",
    "import sys\n",
    "sys.path.insert(0, '../pytorch/')\n",
    "from train import *\n",
    "from lr_analyzer import *\n",
    "from criterion import *\n",
    "\n",
    "from Unet import UNet\n",
    "from dataset import *\n",
    "\n",
    "# Small example of network outputs\n",
    "\n",
    "# Global import\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from dataset import *\n",
    "import sys\n",
    "from train import *\n",
    "from lr_analyzer import *\n",
    "from criterion import *\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# Local import\n",
    "\n",
    "print(sys.path)\n",
    "from Unet import UNet\n",
    "\n",
    "from dataset import *\n",
    "data_dir = 'Data1/'\n",
    "# data_dir = 'Training/'\n",
    "SetSz = len(os.listdir(data_dir))\n",
    "print(SetSz )\n",
    "from train import *\n",
    "\n",
    "                                                           \n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from dataset import psf_dataset, splitDataLoader, ToTensor, Normalize\n",
    "from utils_visdom import VisdomWebServer\n",
    "import aotools\n",
    "from criterion import *\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function to be optimized by Optuna.\n",
    "\n",
    "    Hyperparameters chosen to be optimized: optimizer, learning rate,\n",
    "    dropout values, number of convolutional layers, number of filters of\n",
    "    convolutional layers, number of neurons of fully connected layers.\n",
    "\n",
    "    Inputs:\n",
    "        - trial (optuna.trial._trial.Trial): Optuna trial\n",
    "    Returns:\n",
    "        - accuracy(torch.Tensor): The test accuracy. Parameter to be maximized.\n",
    "    \"\"\"\n",
    "    model_dir='Optuna'\n",
    "    os.system('rm -r '+model_dir)\n",
    "    os.system('mkdir '+model_dir)\n",
    "    data_dir = '../generation/Data1/'\n",
    "    # data_dir = 'Training/'\n",
    "    SetSz =64 #len(os.listdir(data_dir))\n",
    "    dataset_size = SetSz\n",
    "    dataset = psf_dataset(root_dir = data_dir, \n",
    "                      size = dataset_size,\n",
    "                      transform = transforms.Compose([Noise(), Normalize(), ToTensor()]))\n",
    "    criterion = RMSELoss()\n",
    "#     batch = trial.suggest_int(\"batch\", int(SetSz/2), 2*SetSz)  # Optimizers\n",
    "    batch=64\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])  # Optimizers\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)     # Learning rates\n",
    "    split = trial.suggest_float(\"split\", 0.7, 0.99)     # Learning rates\n",
    "    stpsz = trial.suggest_int(\"stpsz\", 80, 170)     # Learning rates\n",
    "    gmma = trial.suggest_float(\"gmma\",0.1, 0.9)     # Learning rates\n",
    "    model = UNet(2, 1)\n",
    "    if optimizer_name==\"Adam\":\n",
    "         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if optimizer_name==\"SGD\":\n",
    "         mom = trial.suggest_float(\"mom\", 0.1, 0.99)     # Learning rates\n",
    "    if optimizer_name==\"SGD\":\n",
    "         mom = trial.suggest_float(\"mom\", 0.1, 0.99)     # Learning rates\n",
    "         optimizer = optim.SGD(model.parameters(), lr=lr, momentum=mom)\n",
    "\n",
    "    # Generate the model\n",
    "\n",
    "    # Generate the optimizers\n",
    "\n",
    "    \n",
    "\n",
    "    # Training of the model\n",
    "    for epoch in range(n_epochs):\n",
    "        model, accuracy=train(model, model_dir, dataset, optimizer, criterion, split=[split, 1-split], batch_size=32, stpsz=stpsz, gmma=gmma, epoch=epoch) # Train the model\n",
    "\n",
    "        # For pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def clbk(Study, trial):  \n",
    "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
    "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
    "    df = df.drop('state', axis=1)                 # Exclude state column\n",
    "    df = df.sort_values('value')                  # Sort based on accuracy\n",
    "    df.to_csv('optuna_resultsNew.csv', index=False)  # Save to csv file\n",
    "    if trial.number%5==0:\n",
    "        obj=plot_optimization_history(study)\n",
    "\n",
    "        # plt.savefig('OptHist.png')\n",
    "\n",
    "        pi.write_image(obj, 'OptHist2.png')\n",
    "\n",
    "        obj1=plot_slice(study)\n",
    "        pi.write_image(obj1, 'OpSlice2.png')\n",
    "\n",
    "        obj2=plot_contour(study)\n",
    "        pi.write_image(obj2, 'OptCont2.png')\n",
    "\n",
    "         \n",
    "    \n",
    "def train(model, model_dir, dataset, optimizer, criterion, split=[0.9, 0.1], batch_size=32, stpsz=150, gmma=0.1, epoch=0):\n",
    "\n",
    "#     # Logging\n",
    "#     log_path = os.path.join(model_dir, 'logs.log')\n",
    "#     os.system('nano '+log_path)\n",
    "#     utils.set_logger(log_path)\n",
    "\n",
    "\n",
    "    # ---\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=stpsz, gamma=gmma)\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "\n",
    "    }\n",
    "    # Device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    # Training\n",
    "    since = time.time()\n",
    "    dataset_size = {\n",
    "        'train':int(split[0]*len(dataset)),\n",
    "        'val':int(split[1]*len(dataset))\n",
    "    }\n",
    "\n",
    "    # Dataset\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'], dataloaders['val'] = splitDataLoader(dataset, split=split, \n",
    "                                                             batch_size=batch_size, random_seed=random_seed)\n",
    "    \n",
    "    \n",
    "    best_loss = 0.0\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    zernike_loss =0.0\n",
    "    phase='train'\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "        else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "        for _, sample in enumerate(dataloaders[phase]):\n",
    "              \n",
    "                # GPU support \n",
    "                inputs = sample['image'].to(device)\n",
    "                phase_0 = sample['phase'].to(device)\n",
    "\n",
    "                if (not inputs[0].isnan().any()):\n",
    "\n",
    "                   # zero the parameter gradients\n",
    "                   optimizer.zero_grad()\n",
    "                   #logging.info(' individual loss: %f %f' % (phase_0, phase_estimation))\n",
    "                   # forward: track history if only in train\n",
    "                   with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                       # Network return phase and zernike coeffs\n",
    "                       phase_estimation = model(inputs)\n",
    "                       rgg=torch.squeeze(phase_estimation)\n",
    "                       loss = criterion(torch.squeeze(phase_estimation), phase_0)\n",
    "                       if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()   \n",
    "                       if loss.item()!=np.nan:\n",
    "                             running_loss += 1 * loss.item() * inputs[0].size(0)    \n",
    "#                        logging.info(' individual loss: %f %f' % (loss.item(), \n",
    "#                                                                  .size(0)))\n",
    "                metrics[phase+'_loss'].append(running_loss / dataset_size[phase])\n",
    "                if phase == 'val':\n",
    "                   scheduler.step()\n",
    "                   # Save weigths\n",
    "                   if epoch == 0 or running_loss < best_loss:\n",
    "                        best_loss = running_loss\n",
    "                        model_path = os.path.join(model_dir, 'model.pth')\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "#     print(np.sum(metrics['val_loss']+metrics['train_loss']))\n",
    "    return model, np.sum(metrics['val_loss']+metrics['train_loss'])\n",
    "\n",
    "            \n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for p in optimizer.param_groups:\n",
    "        lr = p['lr']\n",
    "    return lr                                           \n",
    "                                                           \n",
    "                                                           \n",
    "n_epochs = 10                         # Number of training epochs\n",
    "batch_size_train = 32                 # Batch size for training data\n",
    "batch_size_test = 32                # Batch size for testing data\n",
    "number_of_trials = 200                # Number of Optuna trials\n",
    "limit_obs = True                      # Limit number of observations for faster computation\n",
    "\n",
    "\n",
    "if limit_obs:  # Limit number of observations\n",
    "        number_of_train_examples = 500 * batch_size_train  # Max train observations\n",
    "        number_of_test_examples = 5 * batch_size_test      # Max test observations\n",
    "else:\n",
    "        number_of_train_examples = 60000                   # Max train observations\n",
    "        number_of_test_examples = 10000                    # Max test observations\n",
    "    # -------------------------------------------------------------------------\n",
    "# Make runs repeatable\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False  # Disable cuDNN use of nondeterministic algorithms\n",
    "torch.manual_seed(random_seed)\n",
    "from optuna.visualization import plot_optimization_history, plot_slice, plot_contour\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pi\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=number_of_trials, callbacks=[clbk])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "# Save results to csv file\n",
    "df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
    "df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
    "df = df.drop('state', axis=1)                 # Exclude state column\n",
    "df = df.sort_values('value')                  # Sort based on accuracy\n",
    "df.to_csv('optuna_results1.csv', index=False)  # Save to csv file\n",
    "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "print(most_important_parameters)\n",
    "for key, value in most_important_parameters.items():\n",
    "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_slice, plot_contour\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pi\n",
    "obj=plot_optimization_history(study)\n",
    "obj.show()\n",
    "# plt.savefig('OptHist.png')\n",
    "\n",
    "pi.write_image(obj, 'OptHist.png')\n",
    "\n",
    "obj=plot_slice(study)\n",
    "obj.show()\n",
    "pi.write_image(obj, 'OptHist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=plot_contour(study)\n",
    "obj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator set up\n",
    "\n",
    "import time\n",
    "import aotools\n",
    "from radial import radial_data\n",
    "import numpy as np\n",
    "from scipy import fftpack\n",
    "from astropy.io import fits\n",
    "from soapy import SCI, confParse\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "# Small example of network outputs\n",
    "\n",
    "# # Global import\n",
    "# import sys\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torchvision import transforms\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# Local import\n",
    "# sys.path.insert(0, '../pytorch/models/')\n",
    "# from Unet import UNet\n",
    "\n",
    "sys.path.insert(0, '../src/pytorch/')\n",
    "from dataset import *\n",
    "# ------------------------------------------------------------------------\n",
    "# Generate Point Spread functions from randomly drawn non-common path\n",
    "# aberrations. The aberration follows a 1/f^2 law.\n",
    "# One PSF in focus as well as a PSF out of focus are saved in FITS format\n",
    "# (see astropy docs). The corresponding phase and Zernike Coefficient\n",
    "# are also saved.\n",
    "# ------------------------------------------------------------------------\n",
    "os.system('rm -r Data2')\n",
    "os.mkdir('Data2')\n",
    "out='Data2'\n",
    "np.random.seed(seed=0)\n",
    "# sys.path.insert(0, '../src/pytorch/')\n",
    "SOAPY_CONF = \"psf.yaml\"                         # Soapy config\n",
    "# SOAPY_CONF = \"psf.yaml\"                         # Soapy config\n",
    "gridsize = 128                                  # Pixel size of science camera\n",
    "wavelength = 0.8e-6                             # Observational wavelength\n",
    "diameter = 10                                   # Telescope diameter\n",
    "pixelScale = 0.01                               # [''/px]s\n",
    "WFE=3\n",
    "n_psfs = 15                                     # Number of PSFs\n",
    "n_zernike = 100                                # Number of Zernike polynomials\n",
    "# Number of PSFs\n",
    "                            # Number of Zernike polynomials\n",
    "i_zernike = np.arange(2, n_zernike + 2)         # Zernike polynomial indices (piston excluded)\n",
    "o_zernike= []                                   # Zernike polynomial radial Order, see J. Noll paper :\n",
    "for i in range(1,n_zernike):                    # \"Zernike polynomials and atmospheric turbulence\", 1975\n",
    "    for j in range(i+1):\n",
    "        if len(o_zernike) < n_zernike:\n",
    "            o_zernike.append(i)\n",
    "\n",
    "# Generate randomly Zernike coefficient. By dividing the value\n",
    "# by its radial order we produce a distribution following\n",
    "# the expected 1/f^-2 law.\n",
    "c_zernike = 2 * np.random.random((100*n_psfs, n_zernike)) - 1\n",
    "for j in range(10*n_psfs):\n",
    "    for i in range(n_zernike):\n",
    "        c_zernike[j, i] = c_zernike[j, i] / o_zernike[i]\n",
    "c_zernike = np.array([c_zernike[k, :] / np.abs(c_zernike[k, :]).sum()\n",
    "                      * wavelength*(10**9) for k in range(100*n_psfs)])\n",
    "\n",
    "# Update scientific camera parameters\n",
    "config = confParse.loadSoapyConfig(SOAPY_CONF)\n",
    "config.scis[0].pxlScale = pixelScale\n",
    "config.tel.telDiam = diameter\n",
    "config.calcParams()\n",
    "\n",
    "mask = aotools.circle(config.sim.pupilSize / 2., config.sim.simSize).astype(np.float64)\n",
    "zernike_basis = aotools.zernikeArray(n_zernike + 1, config.sim.pupilSize, norm='rms')\n",
    "\n",
    "psfObj = SCI.PSF(config, nSci=0, mask=mask)\n",
    "\n",
    "psfs_in = np.zeros((n_psfs, psfObj.detector.shape[0], psfObj.detector.shape[1]))\n",
    "psfs_out = np.zeros((n_psfs, psfObj.detector.shape[0], psfObj.detector.shape[1]))\n",
    "\n",
    "defocus = (wavelength / 4) * (10 ** 9) * zernike_basis[3, :, :]\n",
    "\n",
    "t0 = time.time()\n",
    "n_fail = 0\n",
    "vals=0\n",
    "i=0\n",
    "# for i in range(n_pxsfs):\n",
    "while vals<n_psfs:\n",
    "#     WFE=np.range(3, 5, 100)\n",
    "    WFE=np.random.randint(3,6, 1)\n",
    "    aberrations_in = np.squeeze(np.sum(c_zernike[i, :, None, None] * WFE*zernike_basis[1:, :, :], axis=0))\n",
    "    psfs_in[vals, :, :] = np.copy(psfObj.frame(aberrations_in.astype(np.float32)))\n",
    "\n",
    "\n",
    "    # psfs_in[i, :, :] = np.random.poisson(lam=100000*psfs_in[i, :, :], size=None)\n",
    "    # psfs_out[i, :, :] = np.random.poisson(lam=100000*psfs_out[i, :, :], size=None)\n",
    "    \n",
    "    # Save\n",
    "    if (not np.isnan(psfs_in[vals, :, :]).any()):\n",
    "       print('Eg')\n",
    "       outfile = out+\"/psf_\" + str(vals) + \".fits\"\n",
    "       hdu_primary = fits.PrimaryHDU(c_zernike[i, :].astype(np.float32))\n",
    "       hdu_phase = fits.ImageHDU(aberrations_in.astype(np.float32), name='PHASE')\n",
    "       hdu_In = fits.ImageHDU(psfs_in[vals, :, :].astype(np.float32), name='INFOCUS')\n",
    "       hdu_Out = fits.ImageHDU(psfs_in[vals, :, :].astype(np.float32), name='OUTFOCUS')\n",
    "       hdu = fits.HDUList([hdu_primary, hdu_phase, hdu_In, hdu_Out])\n",
    "       hdu.writeto(outfile, overwrite=True)\n",
    "       vals=vals+1\n",
    "    t_soapy = time.time() - t0\n",
    "    i=i+1\n",
    "    print(i)\n",
    "print(vals)\n",
    "print('Propagation and saving finished in {0:2f}s'.format(t_soapy))\n",
    "print('Failed: {0:2f}'.format(n_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna as op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation set\n",
    "import os\n",
    "data_dir = 'Data1/'\n",
    "print(os.listdir(data_dir))\n",
    "dataset_size = 10\n",
    "dataset = psf_dataset(root_dir = data_dir, \n",
    "                      size = dataset_size,\n",
    "                      transform = transforms.Compose([Normalize(), ToTensor()]))\n",
    "\n",
    "# Select one input (here id=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for id in range(dataset_size): \n",
    "    sample = dataset[id]\n",
    "    phase = sample['phase']\n",
    "    image_in = sample['image'][0]\n",
    "    image_out = sample['image'][1]\n",
    "\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(15, 10))\n",
    "    im1 = axarr[0].imshow(phase, cmap=plt.cm.jet)\n",
    "#     im1.set_clim(-np.pi, np.pi)\n",
    "    axarr[0].set_title(\"Phase\")\n",
    "    plt.colorbar(im1, ax = axarr[0], fraction=0.046)\n",
    "    im2 = axarr[1].imshow(image_in, cmap=plt.cm.jet)\n",
    "    axarr[1].set_title(\"In\")\n",
    "    plt.colorbar(im2, ax = axarr[1], fraction=0.046)\n",
    "    im3 = axarr[2].imshow(image_out, cmap=plt.cm.jet)\n",
    "    axarr[2].set_title(\"Out\")\n",
    "    plt.colorbar(im3, ax = axarr[2], fraction=0.046)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# sys.path.insert(0, '../src/pytorch/models/')\n",
    "# print(os.listdir('../src/pytorch/models/'))\n",
    "dls='NewOne4/metrics.json'\n",
    "\n",
    "# dls='mts.json'\n",
    "with open(dls) as f:\n",
    "    file= json.load(f)\n",
    "    trainLoss=file['train_loss']\n",
    "    valLoss= file['val_loss']\n",
    "    lr=file['learning_rate']\n",
    "#     print(json.load(f))\n",
    "#     print(d)\n",
    "Nmbers=np.arange(0, len(trainLoss))\n",
    "plt.scatter(Nmbers, trainLoss)\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(0., 0.05)\n",
    "plt.show()\n",
    "Nmbers=np.arange(0, len(valLoss))\n",
    "# print(valLoss)\n",
    "plt.scatter(Nmbers, valLoss)\n",
    "plt.ylabel('Valuation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "# plt.xlim(50, 300)\n",
    "# plt.ylim(0.05,0.13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA \n",
    "\n",
    "sample=dataset[0]\n",
    "print(sample['image'].unsqueeze(0))\n",
    "def shrink(data, rows, cols):\n",
    "    shrunk = np.zeros((rows,cols))\n",
    "    for i in list(range(0,rows)):\n",
    "        for j in list(range(0,cols)):\n",
    "            row_sp = int(data.shape[0]/rows)\n",
    "            col_sp = int(data.shape[1]/cols)\n",
    "#             print(col_sp, row_sp)\n",
    "            zz = data[i*row_sp : i*row_sp + row_sp, j*col_sp : j*col_sp + col_sp]\n",
    "            shrunk[i,j] = np.sum(zz)\n",
    "    return shrunk\n",
    "im2=plt.imread('../../../../../CNN/ML/src/generation/ar1/run06/Shot22.tif').astype(float)\n",
    "im22=np.loadtxt('../../../../../CNN/ML/src/generation/arNew/ar/sphr33.txt')\n",
    "data=im2\n",
    "sample=dataset[i]\n",
    "phase=sample['phase']\n",
    "input = sample['image'].unsqueeze(0)\n",
    "print(np.shape(input[0][0]))\n",
    "plt.imshow(im2[ 900:1700,1200:2000])\n",
    "fin=(data - data.mean()) / (data.max() - data.min())\n",
    "fin=shrink(fin[ 900:1796,1200:2096], 128, 128)\n",
    "input = torch.from_numpy(np.array([[fin, fin]]))\n",
    "\n",
    "print(input)\n",
    "output = model(input)\n",
    "output_numpy = output.detach().numpy()[0][0]\n",
    "phase_est = output_numpy\n",
    "\n",
    "#     print(phase)\n",
    "# Plot network output\n",
    "# f, axarr = plt.subplots(1, 3, figsize=(15, 10))\n",
    "# im1 = axarr[0].imshow(phase, cmap=plt.cm.jet)\n",
    "# minlim = torch.min(phase)\n",
    "# maxlim=torch.max(phase)\n",
    "# axarr[0].set_title(\"Phase exact\")\n",
    "plt.colorbar(im1, ax = axarr[0], fraction=0.046)\n",
    "im2 = axarr[1].imshow(output_numpy, cmap=plt.cm.jet)\n",
    "im2.set_clim(minlim, maxlim)\n",
    "axarr[1].set_title(\"Phase estimated \")\n",
    "plt.colorbar(im2, ax = axarr[1], fraction=0.046)\n",
    "im3 = axarr[2].imshow(phase.numpy()-output_numpy, cmap=plt.cm.jet)\n",
    "axarr[2].set_title(\"Residual (Exact-Estimated)\")\n",
    "plt.colorbar(im3, ax = axarr[2], fraction=0.046)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model architectures and weights\n",
    "\n",
    "model = UNet(2, 1)\n",
    "# sample=dataset[i]\n",
    "model_dir = 'LR2/model.pth'\n",
    "# model_dir='/Users/jh2619/Desktop/Machine-learning-for-image-based-wavefront-sensing-master/src/pytorch/models/Unet.py'\n",
    "state_dict = torch.load(model_dir, map_location='cpu')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove module.\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network prediction\n",
    "vls=[]\n",
    "# Forward pass and get phase estimation\n",
    "# Need to expand firts dimension to match batch format\n",
    "# [batch]x[channel]x[dim_x]x[dim_y]\n",
    "ids=[0,1,2,3,4]\n",
    "for i in ids:\n",
    "    sample=dataset[i]\n",
    "    phase=sample['phase']\n",
    "    input = sample['image'].unsqueeze(0)\n",
    "    output = model(input)\n",
    "    output_numpy = output.detach().numpy()[0][0]\n",
    "    phase_est = output_numpy\n",
    "#     print(phase)\n",
    "    # Plot network output\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(15, 10))\n",
    "    im1 = axarr[0].imshow(phase, cmap=plt.cm.jet)\n",
    "    minlim = torch.min(phase)\n",
    "    maxlim=torch.max(phase)\n",
    "    axarr[0].set_title(\"Phase exact\")\n",
    "    plt.colorbar(im1, ax = axarr[0], fraction=0.046)\n",
    "    im2 = axarr[1].imshow(output_numpy, cmap=plt.cm.jet)\n",
    "    im2.set_clim(minlim, maxlim)\n",
    "    axarr[1].set_title(\"Phase estimated \")\n",
    "    plt.colorbar(im2, ax = axarr[1], fraction=0.046)\n",
    "    im3 = axarr[2].imshow(phase.numpy()-output_numpy, cmap=plt.cm.jet)\n",
    "    axarr[2].set_title(\"Residual (Exact-Estimated)\")\n",
    "    plt.colorbar(im3, ax = axarr[2], fraction=0.046)\n",
    "    plt.show()\n",
    "    vls.append(np.sum(abs(phase.numpy()-output_numpy)))\n",
    "print(np.mean(vls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project phase on Zernike Basis\n",
    "\n",
    "# 1) Define the basis\n",
    "z_basis = aotools.zernikeArray(101, 128, norm='rms')  # [rad]\n",
    "\n",
    "# 2) Define Standard Dot product\n",
    "def frobeniusInnerProduct(A, B):\n",
    "    fIProduct = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            fIProduct += A[i, j]*B[i, j]\n",
    "    return fIProduct;\n",
    "\n",
    "# 3) Project\n",
    "z_norm = np.empty([100])\n",
    "z_coeffs = np.empty([100])\n",
    "z_coeffs_est = np.empty([100])\n",
    "for i in range(100):\n",
    "    z_norm[i] = frobeniusInnerProduct(z_basis[i+1,:,:], z_basis[i+1,:,:])\n",
    "    z_coeffs[i] = frobeniusInnerProduct(phase, z_basis[i+1,:,:]) / z_norm[i]\n",
    "    z_coeffs_est[i] = frobeniusInnerProduct(phase_est, z_basis[i+1,:,:]) / z_norm[i]\n",
    "    \n",
    "# 4) Plot\n",
    "colors2 = ['#669f61','#ff884d','#32526e','#81b9c3','#41c3ac']\n",
    "nbr = 30;\n",
    "i_zernike = np.arange(2, nbr+2)\n",
    "f, ax = plt.subplots(2, 1, figsize=(15, 8))\n",
    "w = 0.4\n",
    "ax[0].bar(i_zernike-w/2, np.abs(z_coeffs[0:nbr]),  width=w, label='Original', color=colors2[2],zorder=3)\n",
    "ax[0].bar(i_zernike +w/2, np.abs(z_coeffs_est[0:nbr]),  width=w, label='Estimated', color=colors2[3],zorder=3)\n",
    "ax[0].set_ylabel('Absolute magnitude [Rad]')\n",
    "ax[0].legend(fancybox=False, frameon=False)\n",
    "ax[1].bar(i_zernike-w/2, np.abs(z_coeffs[nbr:2*nbr]),  width=w, label='Original', color=colors2[2],zorder=3)\n",
    "ax[1].bar(i_zernike +w/2, np.abs(z_coeffs_est[nbr:2*nbr]), width=w, label='Estimated', color=colors2[3],zorder=3)\n",
    "ax[1].set_ylabel('Absolute magnitude [Rad]')\n",
    "ax[1].set_xlabel(\"Zernike indices\")\n",
    "ax[1].set_xticks(np.arange(2, nbr+2)[0::4])\n",
    "ax[1].set_xticklabels(np.arange(nbr+2, nbr+nbr+2)[0::4])\n",
    "ax[0].set_ylim(0,0.45)\n",
    "ax[1].set_ylim(0,0.45)\n",
    "ax[0].grid(zorder=0, color='lightgray', linestyle='--')\n",
    "ax[1].grid(zorder=0, color='lightgray', linestyle='--')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src/algorithms/')\n",
    "from utils import *\n",
    "\n",
    "# Wavefront root mean squared error over the phase map (pixel wise) [Rad]\n",
    "rmse_wfe = rootMeanSquaredError(phase, phase_est).numpy()\n",
    "print(\"WFE RMSE: \", rmse_wfe, \"[Rad]\")\n",
    "print(\"WFE RMSE: \", 2200*rmse_wfe/(2*np.pi), \"[nm]\")\n",
    "\n",
    "# Wavefront root mean squared error (wfe rmse) [Rad]\n",
    "rmse_zernike = rootMeanSquaredError(z_coeffs, z_coeffs_est, False)\n",
    "print(\"Zernike RMSE: \", rmse_zernike, \"[Rad]\")\n",
    "print(\"Zernike RMSE: \", 2200*rmse_zernike/(2*np.pi), \"[nm]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
